{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan130103/Data-Science-Projects/blob/main/Deepfake_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu4v7esWiwJK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import dlib\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive to access the CelebDF dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "celeb_df_path = '/content/drive/MyDrive/Colab Notebooks/archive'  # Update with your actual path\n",
        "output_path = '/content/processed_data'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Step 1: Dataset Exploration and Preprocessing\n",
        "def explore_dataset(dataset_path):\n",
        "    \"\"\"Explore the structure and contents of the CelebDF dataset\"\"\"\n",
        "    real_videos = []\n",
        "    fake_videos = []\n",
        "\n",
        "    # Assuming dataset has 'real' and 'fake' directories\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        if 'real' in root.lower():\n",
        "            for file in files:\n",
        "                if file.endswith(('.mp4', '.avi')):\n",
        "                    real_videos.append(os.path.join(root, file))\n",
        "        elif 'fake' in root.lower():\n",
        "            for file in files:\n",
        "                if file.endswith(('.mp4', '.avi')):\n",
        "                    fake_videos.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"Found {len(real_videos)} real videos and {len(fake_videos)} fake videos\")\n",
        "\n",
        "    # Sample a few videos to analyze\n",
        "    sample_real = real_videos[:5] if real_videos else []\n",
        "    sample_fake = fake_videos[:5] if fake_videos else []\n",
        "\n",
        "    video_stats = []\n",
        "\n",
        "    # Analyze sample videos\n",
        "    for video_path in sample_real + sample_fake:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        duration = frame_count / fps if fps > 0 else 0\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        cap.release()\n",
        "\n",
        "        video_stats.append({\n",
        "            'path': video_path,\n",
        "            'type': 'real' if 'real' in video_path.lower() else 'fake',\n",
        "            'fps': fps,\n",
        "            'frame_count': frame_count,\n",
        "            'duration': duration,\n",
        "            'resolution': f\"{width}x{height}\"\n",
        "        })\n",
        "\n",
        "    stats_df = pd.DataFrame(video_stats)\n",
        "    print(\"\\nVideo Statistics:\")\n",
        "    print(stats_df.describe())\n",
        "\n",
        "    return real_videos, fake_videos, stats_df\n",
        "\n",
        "# Step 2: Face Detection and Extraction\n",
        "def extract_faces(video_path, output_folder, max_frames=30):\n",
        "    \"\"\"Extract faces from video frames using dlib\"\"\"\n",
        "    # Create face detector\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    # Create output folder\n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    video_output = os.path.join(output_folder, video_name)\n",
        "    os.makedirs(video_output, exist_ok=True)\n",
        "\n",
        "    # Open video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate sampling interval to evenly distribute frame selection\n",
        "    if total_frames <= max_frames:\n",
        "        frame_indices = range(total_frames)\n",
        "    else:\n",
        "        frame_indices = np.linspace(0, total_frames-1, max_frames, dtype=int)\n",
        "\n",
        "    face_count = 0\n",
        "    frame_metrics = []\n",
        "\n",
        "    # Process frames\n",
        "    for frame_idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            continue\n",
        "\n",
        "        # Convert to grayscale for face detection\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect faces\n",
        "        faces = detector(gray)\n",
        "\n",
        "        # Process each face in the frame\n",
        "        for i, face in enumerate(faces):\n",
        "            x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n",
        "\n",
        "            # Add margin to face bounding box\n",
        "            margin = 30\n",
        "            x1 = max(0, x1 - margin)\n",
        "            y1 = max(0, y1 - margin)\n",
        "            x2 = min(frame.shape[1], x2 + margin)\n",
        "            y2 = min(frame.shape[0], y2 + margin)\n",
        "\n",
        "            # Extract face ROI\n",
        "            face_roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "            # Only save if the face has reasonable dimensions\n",
        "            if face_roi.shape[0] > 50 and face_roi.shape[1] > 50:\n",
        "                face_filename = f\"{video_output}/face_{frame_idx:04d}_{i}.jpg\"\n",
        "                cv2.imwrite(face_filename, face_roi)\n",
        "                face_count += 1\n",
        "\n",
        "                # Calculate basic metrics about the face\n",
        "                face_height, face_width = face_roi.shape[:2]\n",
        "                face_area = face_height * face_width\n",
        "\n",
        "                frame_metrics.append({\n",
        "                    'video': video_name,\n",
        "                    'frame': frame_idx,\n",
        "                    'face_id': i,\n",
        "                    'face_width': face_width,\n",
        "                    'face_height': face_height,\n",
        "                    'face_area': face_area,\n",
        "                    'bbox': (x1, y1, x2, y2)\n",
        "                })\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    return face_count, frame_metrics\n",
        "\n",
        "# Step 3: Facial Landmark Detection\n",
        "def extract_facial_landmarks(image_path):\n",
        "    \"\"\"Extract facial landmarks using dlib's 68-point predictor\"\"\"\n",
        "    # Load the detector and predictor\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    # This path needs to be updated - predictor file needs to be downloaded\n",
        "    predictor_path = \"/content/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "    # Download the shape predictor if it doesn't exist\n",
        "    if not os.path.exists(predictor_path):\n",
        "        !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "        !bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "    predictor = dlib.shape_predictor(predictor_path)\n",
        "\n",
        "    # Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        return None, None\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces\n",
        "    faces = detector(gray)\n",
        "\n",
        "    if not faces:\n",
        "        return img, None\n",
        "\n",
        "    # Get the largest face based on area\n",
        "    largest_face = max(faces, key=lambda rect: rect.width() * rect.height())\n",
        "\n",
        "    # Get facial landmarks\n",
        "    landmarks = predictor(gray, largest_face)\n",
        "\n",
        "    # Convert landmarks to numpy array\n",
        "    points = np.zeros((68, 2), dtype=int)\n",
        "    for i in range(68):\n",
        "        points[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
        "\n",
        "    # Draw landmarks on the image for visualization\n",
        "    for (x, y) in points:\n",
        "        cv2.circle(img, (x, y), 2, (0, 255, 0), -1)\n",
        "\n",
        "    return img, points\n",
        "\n",
        "# Step 4: Feature Extraction - Multi-modal\n",
        "def extract_features_from_face(image_path):\n",
        "    \"\"\"Extract comprehensive features from facial image\"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        return None\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    # 1. Basic image statistics\n",
        "    features['mean_color'] = img.mean(axis=(0, 1)).tolist()\n",
        "    features['std_color'] = img.std(axis=(0, 1)).tolist()\n",
        "\n",
        "    # 2. Face-specific metrics from facial landmarks\n",
        "    _, landmarks = extract_facial_landmarks(image_path)\n",
        "\n",
        "    if landmarks is not None:\n",
        "        # Eye aspect ratio (EAR) - can detect blinking inconsistencies\n",
        "        def eye_aspect_ratio(eye_points):\n",
        "            # Compute the euclidean distances between sets of points\n",
        "            A = np.linalg.norm(eye_points[1] - eye_points[5])\n",
        "            B = np.linalg.norm(eye_points[2] - eye_points[4])\n",
        "            C = np.linalg.norm(eye_points[0] - eye_points[3])\n",
        "            # Return the eye aspect ratio\n",
        "            return (A + B) / (2.0 * C)\n",
        "\n",
        "        # Left eye points (36-41 in dlib's 68-point model)\n",
        "        left_eye = landmarks[36:42]\n",
        "        # Right eye points (42-47 in dlib's 68-point model)\n",
        "        right_eye = landmarks[42:48]\n",
        "\n",
        "        left_ear = eye_aspect_ratio(left_eye)\n",
        "        right_ear = eye_aspect_ratio(right_eye)\n",
        "\n",
        "        features['left_eye_aspect_ratio'] = left_ear\n",
        "        features['right_eye_aspect_ratio'] = right_ear\n",
        "\n",
        "        # Mouth aspect ratio - can detect lip sync issues\n",
        "        mouth_top = landmarks[51]\n",
        "        mouth_bottom = landmarks[57]\n",
        "        mouth_left = landmarks[48]\n",
        "        mouth_right = landmarks[54]\n",
        "\n",
        "        mouth_width = np.linalg.norm(mouth_left - mouth_right)\n",
        "        mouth_height = np.linalg.norm(mouth_top - mouth_bottom)\n",
        "\n",
        "        features['mouth_aspect_ratio'] = mouth_height / mouth_width if mouth_width > 0 else 0\n",
        "\n",
        "        # Symmetry score - comparing left and right side of face\n",
        "        # This is crucial for deepfake detection as many deepfakes have subtle asymmetry\n",
        "        left_eye_center = left_eye.mean(axis=0)\n",
        "        right_eye_center = right_eye.mean(axis=0)\n",
        "\n",
        "        nose_tip = landmarks[30]\n",
        "        face_midpoint = (left_eye_center + right_eye_center) / 2\n",
        "\n",
        "        # Distance between nose and face midpoint (ideally small for real faces)\n",
        "        features['nose_midpoint_offset'] = np.linalg.norm(nose_tip - face_midpoint)\n",
        "\n",
        "        # Check if jaw is symmetrical\n",
        "        jaw_left = landmarks[:8]  # Left side of jaw\n",
        "        jaw_right = landmarks[8:17]  # Right side of jaw\n",
        "\n",
        "        # Flip jaw_right horizontally to compare with jaw_left\n",
        "        face_midline_x = face_midpoint[0]\n",
        "        jaw_right_flipped = np.array([(2*face_midline_x - p[0], p[1]) for p in jaw_right])\n",
        "\n",
        "        # Compute average distance between corresponding points\n",
        "        jaw_asymmetry = np.mean([np.linalg.norm(p1 - p2) for p1, p2 in zip(jaw_left, jaw_right_flipped[::-1])])\n",
        "        features['jaw_asymmetry'] = jaw_asymmetry\n",
        "\n",
        "    # 3. Image quality assessment - deepfakes often have compression artifacts\n",
        "    # Laplacian variance - measure of image blur\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    features['laplacian_variance'] = laplacian_var\n",
        "\n",
        "    # 4. Color consistency checks - helps detect lighting inconsistencies\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    features['hsv_mean'] = hsv.mean(axis=(0, 1)).tolist()\n",
        "    features['hsv_std'] = hsv.std(axis=(0, 1)).tolist()\n",
        "\n",
        "    return features\n",
        "\n",
        "# Step 5: Audio Feature Extraction\n",
        "def extract_audio_features(video_path, output_folder):\n",
        "    \"\"\"Extract audio features from video\"\"\"\n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    audio_path = os.path.join(output_folder, f\"{video_name}_audio.wav\")\n",
        "\n",
        "    # Extract audio using ffmpeg\n",
        "    os.system(f\"ffmpeg -i '{video_path}' -q:a 0 -map a '{audio_path}' -y\")\n",
        "\n",
        "    # Check if audio extraction was successful\n",
        "    if not os.path.exists(audio_path) or os.path.getsize(audio_path) < 1000:\n",
        "        return None\n",
        "\n",
        "    # Load audio file\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=None)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    # Extract MFCCs (Mel-frequency cepstral coefficients)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    features['mfcc_mean'] = np.mean(mfccs, axis=1).tolist()\n",
        "    features['mfcc_std'] = np.std(mfccs, axis=1).tolist()\n",
        "\n",
        "    # Extract spectral centroid\n",
        "    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "    features['spectral_centroid_mean'] = np.mean(spectral_centroids)\n",
        "    features['spectral_centroid_std'] = np.std(spectral_centroids)\n",
        "\n",
        "    # Extract zero crossing rate\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "    features['zcr_mean'] = np.mean(zcr)\n",
        "    features['zcr_std'] = np.std(zcr)\n",
        "\n",
        "    # Extract tempo (beats per minute)\n",
        "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "    features['tempo'] = tempo\n",
        "\n",
        "    # Remove the temporary audio file\n",
        "    os.remove(audio_path)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Main Execution Function for Phase 1\n",
        "def execute_phase1(celeb_df_path, output_path):\n",
        "    \"\"\"Execute all steps of Phase 1\"\"\"\n",
        "    # Step 1: Dataset Exploration\n",
        "    print(\"Step 1: Exploring dataset...\")\n",
        "    real_videos, fake_videos, stats_df = explore_dataset(celeb_df_path)\n",
        "\n",
        "    # Create processing directories\n",
        "    real_faces_dir = os.path.join(output_path, 'real_faces')\n",
        "    fake_faces_dir = os.path.join(output_path, 'fake_faces')\n",
        "    os.makedirs(real_faces_dir, exist_ok=True)\n",
        "    os.makedirs(fake_faces_dir, exist_ok=True)\n",
        "\n",
        "    # Visualize dataset statistics\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(stats_df['fps'], kde=True)\n",
        "    plt.title('FPS Distribution')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.histplot(stats_df['duration'], kde=True)\n",
        "    plt.title('Duration Distribution (seconds)')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    stats_df['resolution_width'] = stats_df['resolution'].apply(lambda x: int(x.split('x')[0]))\n",
        "    stats_df['resolution_height'] = stats_df['resolution'].apply(lambda x: int(x.split('x')[1]))\n",
        "    plt.scatter(stats_df['resolution_width'], stats_df['resolution_height'], c=['blue' if t=='real' else 'red' for t in stats_df['type']])\n",
        "    plt.xlabel('Width')\n",
        "    plt.ylabel('Height')\n",
        "    plt.title('Video Resolutions (Blue=Real, Red=Fake)')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.countplot(x='type', data=stats_df)\n",
        "    plt.title('Real vs Fake Videos')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_path, 'dataset_stats.png'))\n",
        "\n",
        "    # Step 2 & 3: Process a subset of videos for development\n",
        "    # Using a smaller subset for initial development\n",
        "    real_subset = real_videos[:50] if len(real_videos) > 50 else real_videos\n",
        "    fake_subset = fake_videos[:50] if len(fake_videos) > 50 else fake_videos\n",
        "\n",
        "    all_face_metrics = []\n",
        "    all_features = []\n",
        "\n",
        "    print(\"\\nStep 2: Extracting faces from videos...\")\n",
        "    for video_path in tqdm(real_subset + fake_subset):\n",
        "        is_real = 'real' in video_path.lower()\n",
        "        output_dir = real_faces_dir if is_real else fake_faces_dir\n",
        "\n",
        "        # Extract faces from video\n",
        "        face_count, metrics = extract_faces(video_path, output_dir)\n",
        "\n",
        "        if metrics:\n",
        "            for metric in metrics:\n",
        "                metric['is_real'] = is_real\n",
        "                all_face_metrics.append(metric)\n",
        "\n",
        "    # Create a DataFrame with face metrics\n",
        "    face_metrics_df = pd.DataFrame(all_face_metrics)\n",
        "    face_metrics_df.to_csv(os.path.join(output_path, 'face_metrics.csv'), index=False)\n",
        "\n",
        "    # Visualize face extraction results\n",
        "    print(f\"Total faces extracted: {len(face_metrics_df)}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(data=face_metrics_df, x='face_width', hue='is_real', kde=True, common_norm=False)\n",
        "    plt.title('Face Width Distribution')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.histplot(data=face_metrics_df, x='face_height', hue='is_real', kde=True, common_norm=False)\n",
        "    plt.title('Face Height Distribution')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.histplot(data=face_metrics_df, x='face_area', hue='is_real', kde=True, common_norm=False)\n",
        "    plt.title('Face Area Distribution')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.countplot(data=face_metrics_df, x='is_real')\n",
        "    plt.title('Real vs Fake Faces')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_path, 'face_extraction_stats.png'))\n",
        "\n",
        "    # Step 4 & 5: Extract features from faces and audio\n",
        "    print(\"\\nStep 4 & 5: Extracting facial and audio features...\")\n",
        "\n",
        "    # Get all extracted face images\n",
        "    all_face_images = []\n",
        "    for root, _, files in os.walk(real_faces_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.jpg'):\n",
        "                all_face_images.append((os.path.join(root, file), True))\n",
        "\n",
        "    for root, _, files in os.walk(fake_faces_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.jpg'):\n",
        "                all_face_images.append((os.path.join(root, file), False))\n",
        "\n",
        "    # Process a subset of face images to extract features\n",
        "    face_subset = all_face_images[:1000] if len(all_face_images) > 1000 else all_face_images\n",
        "\n",
        "    for face_path, is_real in tqdm(face_subset):\n",
        "        # Extract visual features\n",
        "        visual_features = extract_features_from_face(face_path)\n",
        "\n",
        "        if visual_features:\n",
        "            visual_features['is_real'] = is_real\n",
        "            visual_features['image_path'] = face_path\n",
        "\n",
        "            # Get original video path from face path\n",
        "            video_name = os.path.basename(os.path.dirname(face_path))\n",
        "            video_path = None\n",
        "\n",
        "            if is_real and real_videos:\n",
        "                for v in real_videos:\n",
        "                    if video_name in v:\n",
        "                        video_path = v\n",
        "                        break\n",
        "            elif not is_real and fake_videos:\n",
        "                for v in fake_videos:\n",
        "                    if video_name in v:\n",
        "                        video_path = v\n",
        "                        break\n",
        "\n",
        "            # Extract audio features if video path is found\n",
        "            if video_path:\n",
        "                audio_features = extract_audio_features(video_path, output_path)\n",
        "                if audio_features:\n",
        "                    visual_features.update({f\"audio_{k}\": v for k, v in audio_features.items()})\n",
        "\n",
        "            all_features.append(visual_features)\n",
        "\n",
        "    # Create features DataFrame\n",
        "    if all_features:\n",
        "        # Convert list of dictionaries to DataFrame\n",
        "        features_df = pd.DataFrame([{k: str(v) if isinstance(v, list) else v\n",
        "                                  for k, v in d.items()}\n",
        "                                 for d in all_features])\n",
        "\n",
        "        # Save features to CSV\n",
        "        features_df.to_csv(os.path.join(output_path, 'extracted_features.csv'), index=False)\n",
        "\n",
        "        print(f\"Saved {len(features_df)} feature records to CSV\")\n",
        "    else:\n",
        "        print(\"No features were extracted\")\n",
        "\n",
        "    # Return the paths to the processed data\n",
        "    return {\n",
        "        'real_faces_dir': real_faces_dir,\n",
        "        'fake_faces_dir': fake_faces_dir,\n",
        "        'face_metrics_file': os.path.join(output_path, 'face_metrics.csv'),\n",
        "        'features_file': os.path.join(output_path, 'extracted_features.csv'),\n",
        "        'dataset_stats_image': os.path.join(output_path, 'dataset_stats.png'),\n",
        "        'face_stats_image': os.path.join(output_path, 'face_extraction_stats.png')\n",
        "    }\n",
        "\n",
        "# Execute Phase 1\n",
        "results = execute_phase1(celeb_df_path=celeb_df_path, output_path=output_path)\n",
        "print(\"Phase 1 completed successfully!\")\n",
        "print(f\"Results saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO7atu9WvRSD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import cv2\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from einops import rearrange\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Phase 2: Model Development & Adversarial Training\n",
        "print(\"Starting Phase 2: Model Development & Adversarial Training\")\n",
        "\n",
        "# Define paths from Phase 1\n",
        "output_path = '/content/processed_data'\n",
        "real_faces_dir = os.path.join(output_path, 'real_faces')\n",
        "fake_faces_dir = os.path.join(output_path, 'fake_faces')\n",
        "\n",
        "# Step 1: Create PyTorch Dataset for Deepfake Detection\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, real_dir, fake_dir, transform=None, max_samples=5000, temporal_frames=8):\n",
        "        self.transform = transform\n",
        "        self.temporal_frames = temporal_frames\n",
        "\n",
        "        # Get all image paths\n",
        "        real_images = self._get_image_paths(real_dir)\n",
        "        fake_images = self._get_image_paths(fake_dir)\n",
        "\n",
        "        # Limit dataset size if needed\n",
        "        real_images = real_images[:max_samples] if len(real_images) > max_samples else real_images\n",
        "        fake_images = fake_images[:max_samples] if len(fake_images) > max_samples else fake_images\n",
        "\n",
        "        # Group frames by video for temporal analysis\n",
        "        self.real_videos = self._group_by_video(real_images)\n",
        "        self.fake_videos = self._group_by_video(fake_images)\n",
        "\n",
        "        # Create image-level samples for spatial model\n",
        "        self.images = [(img, 1) for img in real_images] + [(img, 0) for img in fake_images]\n",
        "\n",
        "        # Create video-level samples for temporal model\n",
        "        self.videos = []\n",
        "        for video_frames in self.real_videos.values():\n",
        "            if len(video_frames) >= temporal_frames:\n",
        "                self.videos.append((video_frames, 1))\n",
        "\n",
        "        for video_frames in self.fake_videos.values():\n",
        "            if len(video_frames) >= temporal_frames:\n",
        "                self.videos.append((video_frames, 0))\n",
        "\n",
        "        print(f\"Loaded {len(self.images)} total images, {len(self.videos)} video sequences\")\n",
        "        print(f\"Real videos: {len(self.real_videos)}, Fake videos: {len(self.fake_videos)}\")\n",
        "\n",
        "    def _get_image_paths(self, directory):\n",
        "        return glob(os.path.join(directory, \"**/*.jpg\"), recursive=True)\n",
        "\n",
        "    def _group_by_video(self, image_paths):\n",
        "        videos = {}\n",
        "        for img_path in image_paths:\n",
        "            video_name = os.path.basename(os.path.dirname(img_path))\n",
        "            if video_name not in videos:\n",
        "                videos[video_name] = []\n",
        "            videos[video_name].append(img_path)\n",
        "        return videos\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # For spatial analysis (single frame)\n",
        "        if hasattr(self, 'mode') and self.mode == 'temporal':\n",
        "            video_frames, label = self.videos[idx % len(self.videos)]\n",
        "\n",
        "            # Randomly select consecutive frames\n",
        "            if len(video_frames) <= self.temporal_frames:\n",
        "                selected_frames = video_frames\n",
        "            else:\n",
        "                start_idx = random.randint(0, len(video_frames) - self.temporal_frames)\n",
        "                selected_frames = video_frames[start_idx:start_idx + self.temporal_frames]\n",
        "\n",
        "            # Load and transform frames\n",
        "            frames = []\n",
        "            for frame_path in selected_frames:\n",
        "                frame = cv2.imread(frame_path)\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                if self.transform:\n",
        "                    frame = self.transform(image=frame)[\"image\"]\n",
        "                frames.append(frame)\n",
        "\n",
        "            # Stack frames into tensor\n",
        "            frames_tensor = np.stack(frames, axis=0)\n",
        "            return frames_tensor, label\n",
        "\n",
        "        # Default: spatial mode (single image)\n",
        "        img_path, label = self.images[idx]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def set_mode(self, mode):\n",
        "        \"\"\"Switch between spatial and temporal mode\"\"\"\n",
        "        self.mode = mode\n",
        "        if mode == 'spatial':\n",
        "            print(f\"Dataset in spatial mode with {len(self.images)} samples\")\n",
        "        else:\n",
        "            print(f\"Dataset in temporal mode with {len(self.videos)} samples\")\n",
        "\n",
        "# Step 2: Create transformations & augmentations\n",
        "def get_transforms(mode='train'):\n",
        "    if mode == 'train':\n",
        "        return A.Compose([\n",
        "            A.Resize(224, 224),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.OneOf([\n",
        "                A.GaussianBlur(p=0.5),\n",
        "                A.MotionBlur(p=0.5),\n",
        "            ], p=0.3),\n",
        "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, p=0.3),\n",
        "            A.Normalize(),\n",
        "            A.pytorch.transforms.ToTensorV2(),\n",
        "        ])\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Resize(224, 224),\n",
        "            A.Normalize(),\n",
        "            A.pytorch.transforms.ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "# Step 3: Create Vision Transformer Model for Spatial Analysis\n",
        "class ViTForDeepfake(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        # Load pretrained ViT\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=pretrained)\n",
        "\n",
        "        # Replace classifier head\n",
        "        self.vit.head = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.vit.head.in_features, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vit(x)\n",
        "        return x\n",
        "\n",
        "# Step 4: Create 3D CNN Model for Temporal Analysis\n",
        "class VideoTransformer(nn.Module):\n",
        "    def __init__(self, num_frames=8, pretrained=True):\n",
        "        super().__init__()\n",
        "        # Spatial feature extractor (ResNet backbone)\n",
        "        self.backbone = timm.create_model('resnet18', pretrained=pretrained, features_only=True)\n",
        "\n",
        "        # Get feature dimension\n",
        "        dummy = torch.zeros(1, 3, 224, 224)\n",
        "        features = self.backbone(dummy)\n",
        "        feature_dim = features[-1].shape[1]\n",
        "\n",
        "        # Temporal Transformer\n",
        "        self.temporal_transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=feature_dim,\n",
        "                nhead=8,\n",
        "                dim_feedforward=2048,\n",
        "                dropout=0.1,\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(feature_dim),\n",
        "            nn.Linear(feature_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, t, c, h, w = x.shape\n",
        "        # Reshape for backbone processing\n",
        "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
        "\n",
        "        # Extract spatial features\n",
        "        features = self.backbone(x)\n",
        "        # Use the deepest features\n",
        "        x = features[-1]\n",
        "\n",
        "        # Global average pooling\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = x.view(-1, x.size(1))\n",
        "\n",
        "        # Reshape back to batch, time, features\n",
        "        x = rearrange(x, '(b t) c -> b t c', b=b, t=t)\n",
        "\n",
        "        # Apply temporal transformer\n",
        "        x = self.temporal_transformer(x)\n",
        "\n",
        "        # Global temporal pooling\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Step 5: Adversarial Sample Generation\n",
        "class GAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Generator\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + 0.1 * self.generator(x)  # Residual connection for small perturbations\n",
        "\n",
        "# Step 6: Training Functions\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=5):\n",
        "    best_val_acc = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "        for inputs, labels in pbar:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.float().to(device).view(-1, 1)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward + optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            pred = torch.sigmoid(outputs) > 0.5\n",
        "            train_correct += (pred == labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "            pbar.set_postfix({'loss': loss.item(), 'acc': train_correct / train_total})\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.float().to(device).view(-1, 1)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                pred = torch.sigmoid(outputs) > 0.5\n",
        "                val_correct += (pred == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        # Update learning rate\n",
        "        if scheduler:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}')\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), os.path.join(output_path, 'best_model.pth'))\n",
        "\n",
        "    return history\n",
        "\n",
        "# Step 7: Adversarial Training\n",
        "def adversarial_training(detection_model, adv_model, train_loader, criterion, optimizer, device, alpha=0.5):\n",
        "    detection_model.train()\n",
        "    adv_model.train()\n",
        "\n",
        "    adv_optimizer = optim.Adam(adv_model.parameters(), lr=0.0002)\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f'Adversarial Training'):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.float().to(device).view(-1, 1)\n",
        "\n",
        "        # Generate adversarial samples\n",
        "        adv_optimizer.zero_grad()\n",
        "        adv_inputs = adv_model(inputs)\n",
        "\n",
        "        # Train generator to fool detector\n",
        "        adv_outputs = detection_model(adv_inputs)\n",
        "        # Generator wants to make real images classified as fake\n",
        "        real_indices = (labels == 1).squeeze()\n",
        "        if real_indices.sum() > 0:\n",
        "            adv_loss = criterion(adv_outputs[real_indices], torch.zeros_like(labels[real_indices]))\n",
        "            adv_loss.backward()\n",
        "            adv_optimizer.step()\n",
        "\n",
        "        # Train detector on both original and adversarial samples\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Original samples\n",
        "        outputs = detection_model(inputs)\n",
        "        loss_orig = criterion(outputs, labels)\n",
        "\n",
        "        # Adversarial samples (detached)\n",
        "        with torch.no_grad():\n",
        "            adv_inputs = adv_model(inputs).detach()\n",
        "\n",
        "        adv_outputs = detection_model(adv_inputs)\n",
        "        # For real samples made to look fake, target is still real\n",
        "        loss_adv = criterion(adv_outputs, labels)\n",
        "\n",
        "        # Combined loss\n",
        "        loss = (1-alpha) * loss_orig + alpha * loss_adv\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Step 8: Main training function\n",
        "def execute_phase2(real_faces_dir, fake_faces_dir, output_path):\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_transform = get_transforms(mode='train')\n",
        "    val_transform = get_transforms(mode='val')\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = DeepfakeDataset(real_faces_dir, fake_faces_dir, transform=train_transform)\n",
        "\n",
        "    # Split dataset\n",
        "    train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create subsets\n",
        "    from torch.utils.data import Subset\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    val_dataset = Subset(dataset, val_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "    print(f\"Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples\")\n",
        "\n",
        "    # Step 1: Train ViT Model for Spatial Analysis\n",
        "    print(\"\\nTraining Vision Transformer for Spatial Analysis...\")\n",
        "    vit_model = ViTForDeepfake().to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.AdamW(vit_model.parameters(), lr=2e-5, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    # Train for fewer epochs to showcase the code\n",
        "    vit_history = train_model(\n",
        "        vit_model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=3\n",
        "    )\n",
        "\n",
        "    # Step 2: Switch to temporal mode and train Video Transformer\n",
        "    print(\"\\nTraining Video Transformer for Temporal Analysis...\")\n",
        "    dataset.set_mode('temporal')\n",
        "\n",
        "    # Create new dataloaders\n",
        "    train_temporal_dataset = Subset(dataset, train_indices[:len(train_indices)//2])  # Use a subset for speed\n",
        "    val_temporal_dataset = Subset(dataset, val_indices[:len(val_indices)//2])\n",
        "\n",
        "    train_temporal_loader = DataLoader(train_temporal_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "    val_temporal_loader = DataLoader(val_temporal_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Create and train model\n",
        "    video_model = VideoTransformer().to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.AdamW(video_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    # Train for fewer epochs to showcase the code\n",
        "    video_history = train_model(\n",
        "        video_model, train_temporal_loader, val_temporal_loader, criterion, optimizer, scheduler, device, num_epochs=2\n",
        "    )\n",
        "\n",
        "    # Step 3: Create adversarial training setup\n",
        "    print(\"\\nPerforming Adversarial Training...\")\n",
        "    dataset.set_mode('spatial')  # Switch back to spatial mode\n",
        "\n",
        "    # Create adversarial model\n",
        "    adv_model = GAN().to(device)\n",
        "\n",
        "    # Train with adversarial samples\n",
        "    # Reset optimizer for new training phase\n",
        "    optimizer = optim.AdamW(vit_model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
        "\n",
        "    # Do one epoch of adversarial training\n",
        "    adversarial_training(vit_model, adv_model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Step 4: Ensemble the models and evaluate\n",
        "    print(\"\\nEvaluating ensemble model...\")\n",
        "\n",
        "    # Switch to validation mode\n",
        "    dataset.set_mode('spatial')\n",
        "\n",
        "    # Save models\n",
        "    torch.save(vit_model.state_dict(), os.path.join(output_path, 'vit_model.pth'))\n",
        "    torch.save(video_model.state_dict(), os.path.join(output_path, 'video_model.pth'))\n",
        "    torch.save(adv_model.state_dict(), os.path.join(output_path, 'adversarial_model.pth'))\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    vit_model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = vit_model(inputs)\n",
        "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds).flatten()\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(os.path.join(output_path, 'roc_curve.png'))\n",
        "\n",
        "    # Convert predictions to binary\n",
        "    binary_preds = (all_preds > 0.5).astype(int)\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, binary_preds, target_names=['Fake', 'Real']))\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(all_labels, binary_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = [0, 1]\n",
        "    plt.xticks(tick_marks, ['Fake', 'Real'])\n",
        "    plt.yticks(tick_marks, ['Fake', 'Real'])\n",
        "\n",
        "    # Add text annotations\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_path, 'confusion_matrix.png'))\n",
        "\n",
        "    print(f\"\\nPhase 2 completed successfully! Results saved to {output_path}\")\n",
        "    return {\n",
        "        'vit_model_path': os.path.join(output_path, 'vit_model.pth'),\n",
        "        'video_model_path': os.path.join(output_path, 'video_model.pth'),\n",
        "        'adversarial_model_path': os.path.join(output_path, 'adversarial_model.pth'),\n",
        "        'roc_curve': os.path.join(output_path, 'roc_curve.png'),\n",
        "        'confusion_matrix': os.path.join(output_path, 'confusion_matrix.png')\n",
        "    }\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q timm albumentations einops\n",
        "\n",
        "# Execute Phase 2\n",
        "if __name__ == \"__main__\":\n",
        "    results = execute_phase2(real_faces_dir=real_faces_dir, fake_faces_dir=fake_faces_dir, output_path=output_path)\n",
        "    print(\"Phase 2 model development and adversarial training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az0p7vSG-7jP"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio onnxruntime\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import gradio as gr\n",
        "from einops import rearrange\n",
        "import time\n",
        "import shutil\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Phase 3: Deployment & Real-World Testing\n",
        "print(\"Starting Phase 3: Deployment & Real-World Testing\")\n",
        "\n",
        "# Define paths from previous phases\n",
        "output_path = '/content/processed_data'\n",
        "models_path = output_path\n",
        "\n",
        "# Step 1: Load models from Phase 2\n",
        "class ViTForDeepfake(nn.Module):\n",
        "    def __init__(self, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=pretrained)\n",
        "        self.vit.head = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.vit.head.in_features, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "class VideoTransformer(nn.Module):\n",
        "    def __init__(self, num_frames=8, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model('resnet18', pretrained=pretrained, features_only=True)\n",
        "\n",
        "        # Get feature dimension\n",
        "        dummy = torch.zeros(1, 3, 224, 224)\n",
        "        features = self.backbone(dummy)\n",
        "        feature_dim = features[-1].shape[1]\n",
        "\n",
        "        self.temporal_transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=feature_dim,\n",
        "                nhead=8,\n",
        "                dim_feedforward=2048,\n",
        "                dropout=0.1,\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(feature_dim),\n",
        "            nn.Linear(feature_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, t, c, h, w = x.shape\n",
        "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
        "\n",
        "        features = self.backbone(x)\n",
        "        x = features[-1]\n",
        "\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = x.view(-1, x.size(1))\n",
        "\n",
        "        x = rearrange(x, '(b t) c -> b t c', b=b, t=t)\n",
        "\n",
        "        x = self.temporal_transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        return self.classifier(x)\n",
        "\n",
        "def load_models():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load spatial model\n",
        "    spatial_model = ViTForDeepfake().to(device)\n",
        "    spatial_model.load_state_dict(torch.load(os.path.join(models_path, 'vit_model.pth'),\n",
        "                                            map_location=device))\n",
        "    spatial_model.eval()\n",
        "\n",
        "    # Load temporal model\n",
        "    temporal_model = VideoTransformer().to(device)\n",
        "    temporal_model.load_state_dict(torch.load(os.path.join(models_path, 'video_model.pth'),\n",
        "                                             map_location=device))\n",
        "    temporal_model.eval()\n",
        "\n",
        "    return spatial_model, temporal_model, device\n",
        "\n",
        "# Step 2: Define preprocessing functions\n",
        "def get_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "def preprocess_image(image, transform):\n",
        "    if isinstance(image, str):\n",
        "        image = cv2.imread(image)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return transform(image=image)[\"image\"]\n",
        "\n",
        "def extract_frames(video_path, max_frames=32):\n",
        "    frames = []\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate frame sampling to get evenly distributed frames\n",
        "    if total_frames <= max_frames:\n",
        "        frame_indices = range(total_frames)\n",
        "    else:\n",
        "        frame_indices = np.linspace(0, total_frames-1, max_frames, dtype=int)\n",
        "\n",
        "    for i in frame_indices:\n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        success, frame = vidcap.read()\n",
        "        if success:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "\n",
        "    vidcap.release()\n",
        "    return frames\n",
        "\n",
        "# Step 3: Define detection functions\n",
        "def detect_image(image_path, model, device, transform):\n",
        "    image = preprocess_image(image_path, transform)\n",
        "    image = image.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        prob = torch.sigmoid(output).item()\n",
        "\n",
        "    return prob\n",
        "\n",
        "def detect_video(video_path, spatial_model, temporal_model, device, transform, temporal_window=8):\n",
        "    # Extract frames\n",
        "    frames = extract_frames(video_path)\n",
        "    if len(frames) == 0:\n",
        "        return {\"error\": \"No frames could be extracted from the video\"}\n",
        "\n",
        "    # Process frames\n",
        "    processed_frames = [preprocess_image(frame, transform) for frame in frames]\n",
        "    processed_frames = torch.stack(processed_frames).to(device)\n",
        "\n",
        "    # Spatial analysis (frame by frame)\n",
        "    spatial_probs = []\n",
        "    for i in range(len(processed_frames)):\n",
        "        with torch.no_grad():\n",
        "            output = spatial_model(processed_frames[i].unsqueeze(0))\n",
        "            prob = torch.sigmoid(output).item()\n",
        "            spatial_probs.append(prob)\n",
        "\n",
        "    # Temporal analysis (in windows)\n",
        "    temporal_probs = []\n",
        "    if len(frames) >= temporal_window:\n",
        "        for i in range(0, len(frames) - temporal_window + 1, max(1, temporal_window // 2)):\n",
        "            clip = processed_frames[i:i+temporal_window]\n",
        "            if len(clip) < temporal_window:\n",
        "                # Pad if necessary\n",
        "                padding = temporal_window - len(clip)\n",
        "                clip = torch.cat([clip, clip[-1].unsqueeze(0).repeat(padding, 1, 1, 1)])\n",
        "\n",
        "            clip = clip.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = temporal_model(clip)\n",
        "                prob = torch.sigmoid(output).item()\n",
        "                temporal_probs.append(prob)\n",
        "\n",
        "    # Ensemble results\n",
        "    if temporal_probs:\n",
        "        final_prob = 0.6 * np.mean(spatial_probs) + 0.4 * np.mean(temporal_probs)\n",
        "    else:\n",
        "        final_prob = np.mean(spatial_probs)\n",
        "\n",
        "    # Prepare visualization of frame-by-frame analysis\n",
        "    fig, ax = plt.subplots(figsize=(10, 4))\n",
        "    ax.plot(spatial_probs, '-o', label='Frame scores')\n",
        "    ax.axhline(y=0.5, color='r', linestyle='--', label='Threshold')\n",
        "    ax.set_title(f\"Final score: {final_prob:.4f} ({'Real' if final_prob > 0.7 else 'Fake'})\")\n",
        "    ax.set_xlabel('Frame Number')\n",
        "    ax.set_ylabel('Realness Score')\n",
        "    ax.legend()\n",
        "\n",
        "    # Save plot\n",
        "    plot_path = '/tmp/detection_plot.png'\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        \"spatial_scores\": spatial_probs,\n",
        "        \"temporal_scores\": temporal_probs if temporal_probs else None,\n",
        "        \"final_score\": final_prob,\n",
        "        \"prediction\": \"Real\" if final_prob > 0.7 else \"Fake\",\n",
        "        \"confidence\": abs(final_prob - 0.5) * 2,  # Scale to 0-1\n",
        "        \"plot\": plot_path\n",
        "    }\n",
        "\n",
        "# Step 4: Model evaluation on challenging datasets\n",
        "def evaluate_challenging_dataset(models, dataset_path, results_path):\n",
        "    spatial_model, temporal_model, device = models\n",
        "    transform = get_transforms()\n",
        "\n",
        "    # Create results directory\n",
        "    os.makedirs(results_path, exist_ok=True)\n",
        "\n",
        "    # Find all video files\n",
        "    video_files = glob(os.path.join(dataset_path, \"**/*.mp4\"), recursive=True)\n",
        "    image_files = glob(os.path.join(dataset_path, \"**/*.jpg\"), recursive=True) + \\\n",
        "                 glob(os.path.join(dataset_path, \"**/*.png\"), recursive=True)\n",
        "\n",
        "    results = {\"videos\": [], \"images\": []}\n",
        "\n",
        "    # Process videos\n",
        "    for video_path in tqdm(video_files, desc=\"Processing videos\"):\n",
        "        try:\n",
        "            # Determine ground truth from path\n",
        "            is_real = \"real\" in video_path.lower()\n",
        "            label = 1 if is_real else 0\n",
        "\n",
        "            # Run detection\n",
        "            detection = detect_video(video_path, spatial_model, temporal_model, device, transform)\n",
        "\n",
        "            # Store results\n",
        "            results[\"videos\"].append({\n",
        "                \"path\": video_path,\n",
        "                \"label\": label,\n",
        "                \"prediction\": 1 if detection[\"final_score\"] > 0.7 else 0,\n",
        "                \"score\": detection[\"final_score\"]\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {video_path}: {str(e)}\")\n",
        "\n",
        "    # Process images\n",
        "    for image_path in tqdm(image_files, desc=\"Processing images\"):\n",
        "        try:\n",
        "            # Determine ground truth from path\n",
        "            is_real = \"real\" in image_path.lower()\n",
        "            label = 1 if is_real else 0\n",
        "\n",
        "            # Run detection\n",
        "            score = detect_image(image_path, spatial_model, device, transform)\n",
        "\n",
        "            # Store results\n",
        "            results[\"images\"].append({\n",
        "                \"path\": image_path,\n",
        "                \"label\": label,\n",
        "                \"prediction\": 1 if score > 0.7 else 0,\n",
        "                \"score\": score\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {str(e)}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    def calculate_metrics(results_list):\n",
        "        if not results_list:\n",
        "            return None\n",
        "\n",
        "        y_true = [r[\"label\"] for r in results_list]\n",
        "        y_scores = [r[\"score\"] for r in results_list]\n",
        "        y_pred = [r[\"prediction\"] for r in results_list]\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = sum(1 for t, p in zip(y_true, y_pred) if t == p) / len(y_true)\n",
        "\n",
        "        # Calculate ROC\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Calculate precision-recall\n",
        "        precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "        pr_auc = auc(recall, precision)\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"roc_auc\": roc_auc,\n",
        "            \"pr_auc\": pr_auc,\n",
        "            \"fpr\": fpr.tolist(),\n",
        "            \"tpr\": tpr.tolist(),\n",
        "            \"precision\": precision.tolist(),\n",
        "            \"recall\": recall.tolist()\n",
        "        }\n",
        "\n",
        "    metrics = {\n",
        "        \"videos\": calculate_metrics(results[\"videos\"]),\n",
        "        \"images\": calculate_metrics(results[\"images\"])\n",
        "    }\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(results_path, \"evaluation_results.json\"), \"w\") as f:\n",
        "        import json\n",
        "        json.dump({\n",
        "            \"metrics\": metrics,\n",
        "            \"results\": results\n",
        "        }, f)\n",
        "\n",
        "    # Plot ROC curves\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Video ROC\n",
        "    if metrics[\"videos\"]:\n",
        "        ax1.plot(metrics[\"videos\"][\"fpr\"], metrics[\"videos\"][\"tpr\"],\n",
        "                label=f'Video ROC (AUC = {metrics[\"videos\"][\"roc_auc\"]:.3f})')\n",
        "        ax1.plot([0, 1], [0, 1], 'k--')\n",
        "        ax1.set_xlabel('False Positive Rate')\n",
        "        ax1.set_ylabel('True Positive Rate')\n",
        "        ax1.set_title('ROC Curve - Videos')\n",
        "        ax1.legend()\n",
        "\n",
        "    # Image ROC\n",
        "    if metrics[\"images\"]:\n",
        "        ax2.plot(metrics[\"images\"][\"fpr\"], metrics[\"images\"][\"tpr\"],\n",
        "                label=f'Image ROC (AUC = {metrics[\"images\"][\"roc_auc\"]:.3f})')\n",
        "        ax2.plot([0, 1], [0, 1], 'k--')\n",
        "        ax2.set_xlabel('False Positive Rate')\n",
        "        ax2.set_ylabel('True Positive Rate')\n",
        "        ax2.set_title('ROC Curve - Images')\n",
        "        ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_path, \"roc_curves.png\"))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Step 5: Create Gradio interface for demo\n",
        "def create_gradio_interface():\n",
        "    # Load models\n",
        "    spatial_model, temporal_model, device = load_models()\n",
        "    transform = get_transforms()\n",
        "\n",
        "    def process_image(image):\n",
        "        # Process the image\n",
        "        score = detect_image(image, spatial_model, device, transform)\n",
        "\n",
        "        # Create result text\n",
        "        prediction = \"Real\" if score > 0.7 else \"Fake\"\n",
        "        confidence = abs(score - 0.5) * 2  # Scale to 0-1\n",
        "\n",
        "        result_text = f\"Prediction: {prediction}\\nConfidence: {confidence:.2%}\\nScore: {score:.4f}\"\n",
        "\n",
        "        return result_text\n",
        "\n",
        "    def process_video(video):\n",
        "        # Process the video\n",
        "        results = detect_video(video, spatial_model, temporal_model, device, transform)\n",
        "\n",
        "        # Create result text\n",
        "        result_text = f\"Prediction: {results['prediction']}\\n\"\n",
        "        result_text += f\"Confidence: {results['confidence']:.2%}\\n\"\n",
        "        result_text += f\"Score: {results['final_score']:.4f}\"\n",
        "\n",
        "        return result_text, results[\"plot\"] if \"plot\" in results else None\n",
        "\n",
        "    # Create Gradio interface\n",
        "    with gr.Blocks(title=\"Deepfake Detection System\") as demo:\n",
        "        gr.Markdown(\"# Deepfake Detection System\")\n",
        "        gr.Markdown(\"Upload an image or video to detect if it's real or a deepfake.\")\n",
        "\n",
        "        with gr.Tabs():\n",
        "            with gr.TabItem(\"Image Analysis\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        image_input = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
        "                        image_button = gr.Button(\"Analyze Image\")\n",
        "                    with gr.Column():\n",
        "                        image_output = gr.Textbox(label=\"Results\")\n",
        "\n",
        "                image_button.click(process_image, inputs=image_input, outputs=image_output)\n",
        "\n",
        "            with gr.TabItem(\"Video Analysis\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        video_input = gr.Video(label=\"Upload Video\")\n",
        "                        video_button = gr.Button(\"Analyze Video\")\n",
        "                    with gr.Column():\n",
        "                        video_output = gr.Textbox(label=\"Results\")\n",
        "                        plot_output = gr.Image(label=\"Frame Analysis\")\n",
        "\n",
        "                video_button.click(process_video, inputs=video_input, outputs=[video_output, plot_output])\n",
        "\n",
        "        gr.Markdown(\"## How it works\")\n",
        "        gr.Markdown(\"\"\"\n",
        "        This system uses advanced deep learning models to detect manipulated media:\n",
        "\n",
        "        1. **Spatial Analysis**: Analyzes individual frames using Vision Transformers\n",
        "        2. **Temporal Analysis**: Examines inconsistencies across video sequences\n",
        "        3. **Ensemble Approach**: Combines multiple detection signals for robust results\n",
        "\n",
        "        Higher scores (>0.7) indicate the media is likely real, while lower scores suggest manipulation.\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Step 6: Model export for mobile/edge deployment\n",
        "\n",
        "def export_models_for_deployment():\n",
        "    # Load models\n",
        "    spatial_model, temporal_model, device = load_models()\n",
        "\n",
        "    # Create deployment directory\n",
        "    deploy_dir = os.path.join(output_path, \"deployment\")\n",
        "    os.makedirs(deploy_dir, exist_ok=True)\n",
        "\n",
        "    # Export spatial model to ONNX with opset 14\n",
        "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "    try:\n",
        "        torch.onnx.export(\n",
        "            spatial_model,\n",
        "            dummy_input,\n",
        "            os.path.join(deploy_dir, \"spatial_model.onnx\"),\n",
        "            export_params=True,\n",
        "            opset_version=14,  # Changed from 11 to 14\n",
        "            do_constant_folding=True,\n",
        "            input_names=['input'],\n",
        "            output_names=['output'],\n",
        "            dynamic_axes={\n",
        "                'input': {0: 'batch_size'},\n",
        "                'output': {0: 'batch_size'}\n",
        "            }\n",
        "        )\n",
        "        print(\" Spatial model exported successfully with opset 14\")\n",
        "    except Exception as e:\n",
        "        print(f\" Failed to export spatial model: {str(e)}\")\n",
        "        return None\n",
        "    '''\n",
        "def export_models_for_deployment():\n",
        "    # Load models\n",
        "    spatial_model, temporal_model, device = load_models()\n",
        "\n",
        "    # Create deployment directory\n",
        "    deploy_dir = os.path.join(output_path, \"deployment\")\n",
        "    os.makedirs(deploy_dir, exist_ok=True)\n",
        "\n",
        "    # Export to ONNX format\n",
        "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "    torch.onnx.export(\n",
        "        spatial_model,\n",
        "        dummy_input,\n",
        "        os.path.join(deploy_dir, \"spatial_model.onnx\"),\n",
        "        export_params=True,\n",
        "        opset_version=11,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['input'],\n",
        "        output_names=['output'],\n",
        "        dynamic_axes={'input': {0: 'batch_size'},\n",
        "                     'output': {0: 'batch_size'}}\n",
        "    )\n",
        "    '''\n",
        "    # Export metadata\n",
        "    metadata = {\n",
        "        \"input_size\": [224, 224],\n",
        "        \"mean\": [0.485, 0.456, 0.406],\n",
        "        \"std\": [0.229, 0.224, 0.225],\n",
        "        \"threshold\": 0.5\n",
        "    }\n",
        "\n",
        "    # Save metadata\n",
        "    with open(os.path.join(deploy_dir, \"model_metadata.json\"), \"w\") as f:\n",
        "        import json\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    # Create a simple example script for inference\n",
        "   # example_script =\n",
        "    import numpy as np\n",
        "    import cv2\n",
        "    import onnxruntime\n",
        "\n",
        "    def preprocess_image(image_path, size=(224, 224)):\n",
        "        # Read and resize image\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, size)\n",
        "\n",
        "        # Normalize\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        img = (img - mean) / std\n",
        "\n",
        "        # HWC to CHW format\n",
        "        img = img.transpose(2, 0, 1)\n",
        "\n",
        "        # Add batch dimension\n",
        "        img = np.expand_dims(img, 0)\n",
        "        return img\n",
        "\n",
        "    def run_inference(image_path, model_path):\n",
        "        # Load image\n",
        "        input_tensor = preprocess_image(image_path)\n",
        "\n",
        "        # Create ONNX session\n",
        "        session = onnxruntime.InferenceSession(model_path)\n",
        "\n",
        "        # Run inference\n",
        "        input_name = session.get_inputs()[0].name\n",
        "        output_name = session.get_outputs()[0].name\n",
        "        result = session.run([output_name], {input_name: input_tensor})\n",
        "\n",
        "        # Convert to probability\n",
        "        score = 1 / (1 + np.exp(-result[0][0][0]))\n",
        "        prediction = \"Real\" if score > 0.7 else \"Fake\"\n",
        "\n",
        "        print(f\"Prediction: {prediction}\")\n",
        "        print(f\"Score: {score:.4f}\")\n",
        "\n",
        "        return {\"prediction\": prediction, \"score\": float(score)}\n",
        "\n",
        "    # Example usage\n",
        "    if __name__ == \"__main__\":\n",
        "        import sys\n",
        "        if len(sys.argv) < 3:\n",
        "            print(\"Usage: python inference.py <image_path> <model_path>\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        image_path = sys.argv[1]\n",
        "        model_path = sys.argv[2]\n",
        "        result = run_inference(image_path, model_path)\n",
        "\n",
        "\n",
        "    with open(os.path.join(deploy_dir, \"inference.py\"), \"w\") as f:\n",
        "        f.write(example_script.strip())\n",
        "\n",
        "    print(f\"Models exported for deployment to {deploy_dir}\")\n",
        "    return deploy_dir\n",
        "\n",
        "# Main execution function\n",
        "def execute_phase3():\n",
        "    print(\"Beginning Phase 3: Deployment & Real-World Testing\")\n",
        "\n",
        "    # Install required packages\n",
        "    !pip install -q gradio onnxruntime onnx\n",
        "\n",
        "    # Load models\n",
        "    try:\n",
        "        models = load_models()\n",
        "        print(\" Models loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\" Failed to load models: {str(e)}\")\n",
        "        return\n",
        "\n",
        "     # Create deployment package\n",
        "    deploy_dir = None  # Initialize deploy_dir\n",
        "    try:\n",
        "        deploy_dir = export_models_for_deployment() # Assign value here\n",
        "        print(f\" Deployment package created at {deploy_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Failed to create deployment package: {str(e)}\")\n",
        "\n",
        "    # Create benchmark dataset\n",
        "    benchmark_dir = os.path.join(output_path, \"benchmark\")\n",
        "    os.makedirs(benchmark_dir, exist_ok=True)\n",
        "\n",
        "    # Evaluate model on test data\n",
        "    try:\n",
        "        # Check if we have a test dataset\n",
        "        test_dir = '/content/drive/MyDrive/Test'\n",
        "        if os.path.exists(test_dir):\n",
        "            print(\"Evaluating model on test dataset...\")\n",
        "            metrics = evaluate_challenging_dataset(\n",
        "                models,\n",
        "                test_dir,\n",
        "                os.path.join(output_path, \"evaluation_results\")\n",
        "            )\n",
        "            print(f\" Model evaluation complete. Results saved to {os.path.join(output_path, 'evaluation_results')}\")\n",
        "\n",
        "            # Print summary metrics\n",
        "            print(\"\\n--- Performance Summary ---\")\n",
        "            if metrics[\"images\"]:\n",
        "                print(f\"Image Detection: Accuracy={metrics['images']['accuracy']:.4f}, ROC-AUC={metrics['images']['roc_auc']:.4f}\")\n",
        "            if metrics[\"videos\"]:\n",
        "                print(f\"Video Detection: Accuracy={metrics['videos']['accuracy']:.4f}, ROC-AUC={metrics['videos']['roc_auc']:.4f}\")\n",
        "        else:\n",
        "            print(\"No test dataset found. Skipping evaluation.\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error during model evaluation: {str(e)}\")\n",
        "\n",
        "    # Create and launch demo interface\n",
        "    try:\n",
        "        print(\"Creating demo interface...\")\n",
        "        demo = create_gradio_interface()\n",
        "        demo.launch(share=True)\n",
        "        print(\" Demo interface launched\")\n",
        "    except Exception as e:\n",
        "        print(f\" Failed to launch demo: {str(e)}\")\n",
        "\n",
        "    print(\"\\nPhase 3 completed successfully!\")\n",
        "    return {\n",
        "        \"deployment_dir\": deploy_dir,\n",
        "        \"evaluation_results\": os.path.join(output_path, \"evaluation_results\") if os.path.exists(test_dir) else None,\n",
        "        \"demo_interface\": \"Running in Gradio\"\n",
        "    }\n",
        "\n",
        "# Execute Phase 3\n",
        "if __name__ == \"__main__\":\n",
        "    execute_phase3()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}